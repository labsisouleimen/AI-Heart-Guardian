import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Flatten, Dense, Dropout
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib

# ğŸ“Œ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
normal_data = np.loadtxt('normali_ecg_segments.csv', delimiter=',')
abnormal_data = np.loadtxt('abnormali_ecg_segments.csv', delimiter=',')

# ğŸ“Œ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª (Labels)
normal_labels = np.zeros((normal_data.shape[0], 1))  # 0 = Ø·Ø¨ÙŠØ¹ÙŠ
abnormal_labels = np.ones((abnormal_data.shape[0], 1))  # 1 = ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ

# ğŸ“Œ Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ³Ù…ÙŠØ§Øª
X = np.vstack((normal_data, abnormal_data))  # Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª
y = np.vstack((normal_labels, abnormal_labels))  # Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª

# ğŸ“Œ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ (80%) ÙˆØ§Ø®ØªØ¨Ø§Ø± (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ğŸ“Œ ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ğŸ”¹ Ø­ÙØ¸ StandardScaler Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤
joblib.dump(scaler, "scaler.pkl")

# ğŸ“Œ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªÙ†Ø§Ø³Ø¨ Conv1D + LSTM (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø«Ù„Ø§Ø«ÙŠØ© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# ğŸ“Œ ØªØ­ÙˆÙŠÙ„ y Ø¥Ù„Ù‰ One-Hot Encoding
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)

# ğŸ“Œ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Conv1D + LSTM
model = Sequential([
    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(1800, 1)),
    MaxPooling1D(pool_size=2),

    Conv1D(filters=128, kernel_size=5, activation='relu'),
    MaxPooling1D(pool_size=2),

    LSTM(64, return_sequences=True),
    LSTM(32),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    
    Dense(64, activation='relu'),
    Dense(2, activation='softmax')  # Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ (0 = Ø·Ø¨ÙŠØ¹ÙŠØŒ 1 = ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ)
])

# ğŸ“Œ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# ğŸ“Œ Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model.summary()
# ğŸ“Œ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)

# ğŸ“Œ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'ğŸ”¹ Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_accuracy * 100:.2f}%')

# ğŸ”¹ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§
model.save("ecg_cnn_lstm_model2.h5")
